2026-01-23 21:14:43,215 - AppLogger - [INFO] - main.py:247 - Starting the data processing pipeline.
2026-01-23 21:14:43,216 - AppLogger - [INFO] - main.py:248 - Parsed Arguments: {'LLM': 'meta-llama/Meta-Llama-3-8B-Instruct', 'base_raw_data_dir': '/home/guy_b/big-storage/raw_data', 'train_dataset': 'imdb', 'test_dataset': 'imdb_test', 'base_pre_processed_data_dir': '/home/zina.assi/Deep-Learning-project---Hallucination-Detection/data/pre_processed_data', 'probe_model': 'ImprovedLOSNet', 'topk_preprocess': 1000, 'input_output_type': 'output', 'topk_dim': 1000, 'N_max': 100, 'num_folds': 5, 'fold_to_run': 0, 'input_type': 'LOS', 'seed': 0, 'cuda_idx': 0, 'batch_size': 64, 'hidden_dim': 128, 'heads': 8, 'dropout': 0.1, 'num_layers': 2, 'pool': 'cls', 'patience': 20, 'num_epochs': 100, 'best_model_path': 'saved_models', 'lr': 0.0001, 'rank_encoding': 'scale_encoding', 'weight_decay': 0.001, 'num_workers': 4, 'pin_memory': 1, 'rank_embed_dim': 128, 'use_rank_embed': True, 'use_entropy': True, 'use_gaps': True, 'distill_alpha': 0.5, 'distill_temp': 2.0}
2026-01-23 21:14:43,217 - AppLogger - [INFO] - main.py:255 - Loading preproccessed data for model 'meta-llama/Meta-Llama-3-8B-Instruct'
2026-01-23 21:14:43,218 - AppLogger - [INFO] - main.py:25 - Starting data preparation for model 'meta-llama/Meta-Llama-3-8B-Instruct' using training dataset 'imdb'.
2026-01-23 21:14:43,231 - AppLogger - [INFO] - main.py:91 - Training dataset loaded successfully.
2026-01-23 21:14:43,290 - AppLogger - [INFO] - main.py:100 - Test dataset loaded successfully.
2026-01-23 21:14:43,291 - AppLogger - [INFO] - main.py:102 - Dataset processing pipeline completed successfully.
2026-01-23 21:14:43,292 - AppLogger - [INFO] - main.py:260 - Splitting dataset into train, validation, and test indices.
Extracting labels:   0%|          | 0/1000 [00:00<?, ?it/s]Extracting labels:   0%|          | 1/1000 [00:00<03:54,  4.26it/s]Extracting labels:   1%|▏         | 13/1000 [00:00<00:21, 46.32it/s]Extracting labels:   3%|▎         | 27/1000 [00:00<00:12, 75.61it/s]Extracting labels:   4%|▎         | 37/1000 [00:00<00:12, 78.83it/s]Extracting labels:   5%|▍         | 46/1000 [00:00<00:11, 80.87it/s]Extracting labels:   6%|▌         | 56/1000 [00:00<00:11, 85.63it/s]Extracting labels:   7%|▋         | 69/1000 [00:00<00:09, 93.62it/s]Extracting labels:   8%|▊         | 82/1000 [00:01<00:08, 102.75it/s]Extracting labels:   9%|▉         | 93/1000 [00:01<00:08, 102.13it/s]Extracting labels:  10%|█         | 105/1000 [00:01<00:08, 106.02it/s]Extracting labels:  12%|█▏        | 116/1000 [00:01<00:08, 104.74it/s]Extracting labels:  13%|█▎        | 129/1000 [00:01<00:07, 110.24it/s]Extracting labels:  14%|█▍        | 141/1000 [00:01<00:07, 108.72it/s]Extracting labels:  15%|█▌        | 152/1000 [00:01<00:07, 106.65it/s]Extracting labels:  16%|█▋        | 163/1000 [00:01<00:07, 105.67it/s]Extracting labels:  17%|█▋        | 174/1000 [00:01<00:08, 97.70it/s] Extracting labels:  18%|█▊        | 184/1000 [00:02<00:08, 93.46it/s]Extracting labels:  19%|█▉        | 194/1000 [00:02<00:09, 82.46it/s]Extracting labels:  20%|██        | 203/1000 [00:02<00:09, 83.68it/s]Extracting labels:  22%|██▏       | 215/1000 [00:02<00:08, 91.57it/s]Extracting labels:  23%|██▎       | 226/1000 [00:02<00:08, 92.60it/s]Extracting labels:  24%|██▎       | 236/1000 [00:02<00:09, 84.61it/s]Extracting labels:  25%|██▍       | 249/1000 [00:02<00:07, 94.99it/s]Extracting labels:  26%|██▌       | 261/1000 [00:02<00:07, 100.92it/s]Extracting labels:  27%|██▋       | 272/1000 [00:03<00:08, 88.49it/s] Extracting labels:  28%|██▊       | 282/1000 [00:03<00:08, 83.56it/s]Extracting labels:  30%|██▉       | 296/1000 [00:03<00:07, 94.87it/s]Extracting labels:  31%|███       | 308/1000 [00:03<00:06, 99.34it/s]Extracting labels:  32%|███▏      | 319/1000 [00:03<00:07, 89.36it/s]Extracting labels:  33%|███▎      | 329/1000 [00:03<00:08, 79.78it/s]Extracting labels:  34%|███▍      | 340/1000 [00:03<00:07, 85.85it/s]Extracting labels:  35%|███▌      | 351/1000 [00:03<00:07, 91.48it/s]Extracting labels:  36%|███▌      | 361/1000 [00:04<00:07, 82.56it/s]Extracting labels:  37%|███▋      | 373/1000 [00:04<00:07, 86.75it/s]Extracting labels:  38%|███▊      | 385/1000 [00:04<00:06, 95.00it/s]Extracting labels:  40%|████      | 401/1000 [00:04<00:05, 110.34it/s]Extracting labels:  41%|████▏     | 414/1000 [00:04<00:05, 115.11it/s]Extracting labels:  43%|████▎     | 426/1000 [00:04<00:05, 108.14it/s]Extracting labels:  44%|████▍     | 443/1000 [00:04<00:04, 123.17it/s]Extracting labels:  46%|████▌     | 457/1000 [00:04<00:04, 126.26it/s]Extracting labels:  47%|████▋     | 470/1000 [00:04<00:04, 126.59it/s]Extracting labels:  49%|████▊     | 487/1000 [00:05<00:03, 136.87it/s]Extracting labels:  50%|█████     | 503/1000 [00:05<00:03, 142.02it/s]Extracting labels:  52%|█████▏    | 519/1000 [00:05<00:03, 145.30it/s]Extracting labels:  54%|█████▎    | 535/1000 [00:05<00:03, 147.20it/s]Extracting labels:  55%|█████▌    | 551/1000 [00:05<00:02, 150.00it/s]Extracting labels:  57%|█████▋    | 567/1000 [00:05<00:03, 141.01it/s]Extracting labels:  58%|█████▊    | 583/1000 [00:05<00:02, 146.12it/s]Extracting labels:  60%|█████▉    | 598/1000 [00:05<00:03, 132.15it/s]Extracting labels:  61%|██████    | 612/1000 [00:05<00:03, 127.64it/s]Extracting labels:  63%|██████▎   | 627/1000 [00:06<00:02, 131.69it/s]Extracting labels:  64%|██████▍   | 642/1000 [00:06<00:02, 126.53it/s]Extracting labels:  66%|██████▌   | 657/1000 [00:06<00:02, 130.80it/s]Extracting labels:  67%|██████▋   | 671/1000 [00:06<00:02, 128.45it/s]Extracting labels:  69%|██████▊   | 687/1000 [00:06<00:02, 135.31it/s]Extracting labels:  70%|███████   | 701/1000 [00:06<00:02, 124.20it/s]Extracting labels:  72%|███████▏  | 717/1000 [00:06<00:02, 132.81it/s]Extracting labels:  73%|███████▎  | 731/1000 [00:06<00:02, 126.92it/s]Extracting labels:  74%|███████▍  | 744/1000 [00:06<00:02, 127.69it/s]Extracting labels:  76%|███████▌  | 761/1000 [00:07<00:01, 137.09it/s]Extracting labels:  78%|███████▊  | 777/1000 [00:07<00:01, 141.07it/s]Extracting labels:  79%|███████▉  | 792/1000 [00:07<00:01, 133.75it/s]Extracting labels:  81%|████████  | 808/1000 [00:07<00:01, 139.25it/s]Extracting labels:  82%|████████▏ | 823/1000 [00:07<00:01, 134.23it/s]Extracting labels:  84%|████████▎ | 837/1000 [00:07<00:01, 127.54it/s]Extracting labels:  85%|████████▌ | 850/1000 [00:07<00:01, 118.42it/s]Extracting labels:  87%|████████▋ | 866/1000 [00:07<00:01, 128.83it/s]Extracting labels:  88%|████████▊ | 880/1000 [00:07<00:00, 123.17it/s]Extracting labels:  90%|████████▉ | 895/1000 [00:08<00:00, 127.08it/s]Extracting labels:  91%|█████████ | 908/1000 [00:08<00:00, 108.26it/s]Extracting labels:  92%|█████████▏| 920/1000 [00:08<00:00, 102.39it/s]Extracting labels:  94%|█████████▎| 935/1000 [00:08<00:00, 112.30it/s]Extracting labels:  95%|█████████▍| 949/1000 [00:08<00:00, 119.30it/s]Extracting labels:  96%|█████████▌| 962/1000 [00:08<00:00, 121.53it/s]Extracting labels:  98%|█████████▊| 975/1000 [00:08<00:00, 117.51it/s]Extracting labels:  99%|█████████▉| 990/1000 [00:08<00:00, 125.98it/s]Extracting labels: 100%|██████████| 1000/1000 [00:08<00:00, 111.16it/s]
Processing folds:   0%|          | 0/5 [00:00<?, ?it/s]Processing folds: 100%|██████████| 5/5 [00:00<00:00, 47021.35it/s]
2026-01-23 21:14:52,295 - AppLogger - [INFO] - main.py:276 - Train size: 800, Validation size: 200, Test indices: 1000
2026-01-23 21:14:52,341 - AppLogger - [INFO] - main.py:285 - Running fold 1 of 5.
2026-01-23 21:14:52,342 - AppLogger - [INFO] - main.py:287 - Creating dataloaders for training, validation, and test sets.
2026-01-23 21:14:52,351 - AppLogger - [INFO] - main.py:318 - Creating model for input type: LOS with sequence length 100 and feature dimension: 1000
2026-01-23 21:14:52,712 - AppLogger - [INFO] - main.py:327 - Total number of parameters in the model: 450561
2026-01-23 21:14:52,714 - AppLogger - [INFO] - main.py:330 - Creating optimizer and scheduler.
2026-01-23 21:14:52,716 - AppLogger - [INFO] - main.py:335 - Total number of training steps: 1300, and warm-up steps: 130
2026-01-23 21:14:52,717 - AppLogger - [INFO] - main.py:350 - will save the best model in this folder: saved_models/meta-llama/Meta-Llama-3-8B-Instruct/imdb with this file name: 5692717731.
2026-01-23 21:14:52,719 - AppLogger - [INFO] - main.py:351 - Starting wandb, project is LOS-Net.
2026-01-23 21:14:52,720 - AppLogger - [INFO] - main.py:352 - Starting training loop.
2026-01-23 21:14:54,021 - AppLogger - [INFO] - main.py:190 - Epoch 1/100
Training Progress:   0%|          | 0/13 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [33,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [34,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [36,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [43,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [50,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [54,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [55,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [56,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [57,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [0,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.
Training Progress:   0%|          | 0/13 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/zina.assi/Deep-Learning-project---Hallucination-Detection/main.py", line 359, in <module>
    main()
  File "/home/zina.assi/Deep-Learning-project---Hallucination-Detection/main.py", line 356, in main
    train_model(logger=logger, model=model, dataloader_train=dataloader_train, dataloader_val=dataloader_val, dataloader_test=dataloader_test, criterion=criterion, optimizer=optimizer, scheduler=scheduler, args=args, device=device)
  File "/home/zina.assi/Deep-Learning-project---Hallucination-Detection/main.py", line 193, in train_model
    train_loss, auc_train = train_one_epoch(model, dataloader_train, criterion, optimizer, scheduler, device, input_type=args.input_type)
  File "/home/zina.assi/Deep-Learning-project---Hallucination-Detection/main.py", line 132, in train_one_epoch
    loss.backward()
  File "/home/zina.assi/miniconda3/envs/LOS_Net/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/zina.assi/miniconda3/envs/LOS_Net/lib/python3.10/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/zina.assi/miniconda3/envs/LOS_Net/lib/python3.10/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

